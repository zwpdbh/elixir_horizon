# ACStor Replication

## Introduction

### References

* [Steps of replication](https://microsoft.sharepoint.com/teams/AzureStorage/Scenarios/_layouts/15/Doc.aspx?sourcedoc={2fa64dce-8dcf-4568-8d77-2c3c6b5516bf}&action=edit&wd=target%28Storage%20Features%2FGeneral%2FContainer%20and%20AKS.one%7Cfea4ea3b-64a2-4629-a7ad-c7003b84b71f%2FSteps%7C71d3e2c7-541a-4a17-83c4-275279f3ee3c%2F%29&wdorigin=NavigationUrl)
* [How to rebuild](https://microsoft.sharepoint.com/teams/AzureStorage/Scenarios/_layouts/15/Doc.aspx?sourcedoc={2fa64dce-8dcf-4568-8d77-2c3c6b5516bf}&action=edit&wd=target%28Storage%20Features%2FGeneral%2FContainer%20and%20AKS.one%7Cfea4ea3b-64a2-4629-a7ad-c7003b84b71f%2FRebuild%7C531e371e-20dd-4216-bb10-65f9a5638bda%2F%29&wdorigin=NavigationUrl)

<!-- livebook:{"branch_parent_index":0} -->

## 1. Create AKS Cluster

```elixir
# Our global settings 
# subscription has no particular
subscription = "65490f91-f2c2-4514-80ba-4ec1de89aeda"
# region has no particular
region = "eastus"
```

```elixir
# For our repliation test  
disk_type = "azure_disk"
rg = "acstor-replication-test"
aks_cluster = "acstor-replication-test01"
```

```elixir
# Set Azure subscription
ExecCmd.run("az account set --subscription #{subscription}")
```

```elixir
# Create resource group on Azure
ExecCmd.run("az group create  --location #{region} --name #{rg}")
```

```elixir
# Create AKS cluster
ExecCmd.run(
  "az aks create -n #{aks_cluster} -g #{rg} --generate-ssh-keys --attach-acr /subscriptions/d64ddb0c-7399-4529-a2b6-037b33265372/resourceGroups/azstor-test-rg/providers/Microsoft.ContainerRegistry/registries/azstortest"
)
```

```elixir
# Prepare the VM size for node pool
vm_sku =
  case disk_type do
    "azure_disk" -> "Standard_D4s_v3"
    "nvme" -> "Standard_L8s_v3"
  end
```

```elixir
# Add node pool with 3 nodes 
"az aks nodepool add --cluster-name #{aks_cluster} --name storagepool --resource-group #{rg} --node-vm-size #{vm_sku} --node-count 3 "
|> ExecCmd.run()
```

So far, the created AKS cluster is

```elixir
%{
  subscription_id: subscription,
  region: region,
  rg: rg,
  aks: aks_cluster
}
```

<!-- livebook:{"branch_parent_index":0} -->

## 2. Configure AKS Cluster

To execute kubectl command, we need to get the AKS context by overwrite local kubectl config file.

```elixir
aks_settings = %{
  aks: "acstor-replication-test01",
  region: "eastus",
  rg: "acstor-replication-test",
  subscription_id: "65490f91-f2c2-4514-80ba-4ec1de89aeda"
}
```

```elixir
alias Azure.Aks
k8s_config = Aks.get_aks_config(aks_settings)
```

```elixir
File.write("/home/zw/.kube/config", k8s_config)
```

```elixir
# Test k8s context by checking the nodes in AKS cluster
ExecCmd.run("kubectl get nodes")
```

```elixir
# Label the nodes 
"kubectl label nodes --selector agentpool=storagepool acstor.azure.com/io-engine=acstor"
|> ExecCmd.run()
```

```elixir
# show node label 
"kubectl get nodes --show-labels"
|> ExecCmd.run()
```

```elixir
# to remove lablels "acstor.azure.com/io-engine=acstor"
# from node with label = "agentpool=nodepool1" 
"kubectl label nodes --selector agentpool=nodepool1 acstor.azure.com/io-engine-"
|> ExecCmd.run()
```

### Assign contributor role

```elixir
# Check the managed id 
"az aks show -g #{aks_settings.rg} -n #{aks_settings.aks} --out tsv --query identityProfile.kubeletidentity.objectId"
|> ExecCmd.run()
```

```elixir
# check the node resource group 
"az aks show -g #{aks_settings.rg} -n #{aks_settings.aks} --out tsv --query nodeResourceGroup"
|> ExecCmd.run()
```

```elixir
# Obtained from previous 2 steps as ExecCmd's output to terminal
managed_id = "c84af941-c89e-4256-b871-861449a8fc39"
node_rg = "MC_acstor-replication-test_acstor-replication-test01_eastus"
```

```elixir
# Finally, assign the contributor role
"az role assignment create --assignee #{managed_id} --role Contributor --scope /subscriptions/#{aks_settings.subscription_id}/resourceGroups/#{node_rg}"
|> ExecCmd.run()
```

<!-- livebook:{"branch_parent_index":0} -->

## 3. Install ACStor Addons

[azstor-add-ons](https://dev.azure.com/msazure/One/_git/azstor-add-ons) hosts the main Helm chart for ACStor and additional components added by Microsoft.

First, let's check the prerequisites: 
AKS nodes where ACStor should run must be labeled with `acstor.azure.com/io-engine=acstor`.

Notice: the kubectl context is inherited from previous section

<!-- livebook:{"break_markdown":true} -->

###

<!-- livebook:{"break_markdown":true} -->

We need to download the latest add-ons and install it from source code:

```elixir
download_folder = Path.join(["~/download", "azstore-add-ons"])
```

```elixir
"git clone git@ssh.dev.azure.com:v3/msazure/One/azstor-add-ons #{download_folder}"
|> ExecCmd.run()
```

```elixir
# Build add-on
"""
cd #{download_folder}/charts/latest/ && 
helm dependency build
"""
|> ExecCmd.run()
```

```elixir

```

```elixir
"""
cd #{download_folder} && 
helm install acstor charts/latest --namespace acstor --create-namespace \
--version 0.0.0-latest \
--set image.tag=latest \
--set image.registry="azstortest.azurecr.io" \
--set image.repo="mayadata" \
--set capacityProvisioner.image.tag=latest \
--set capacityProvisioner.image.registry="azstortest.azurecr.io"
"""
|> ExecCmd.run()
```

If installation succeed, it should show messages like:

```
AME: acstor
LAST DEPLOYED: Thu Aug 24 14:31:45 2023
NAMESPACE: acstor
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
ACStor has been installed. Check its status by running:
$ kubectl get pods -n acstor
```

So, let check it

```elixir
"kubectl get pods -n acstor"
|> ExecCmd.run()
```

<!-- livebook:{"branch_parent_index":0} -->

## 4. Configure Replication

Notice: the kubectl context is inherited from previous section 2.

<!-- livebook:{"break_markdown":true} -->

Patch the replication images

```elixir
# All these are hardcoded values
# Where are those values come from ？？？
[
  "kubectl set image deployment/acstor-api-rest api-rest=azstortest.azurecr.io/artifact/424bd44c-13b4-4637-a5a4-0b9506e90413/buddy/rest:47c414cef91d05651985c66d6f3bbe317aab35e0-20230809.5 -n acstor",
  "kubectl set image deployment/acstor-agent-core agent-core=azstortest.azurecr.io/artifact/424bd44c-13b4-4637-a5a4-0b9506e90413/buddy/agents.core:47c414cef91d05651985c66d6f3bbe317aab35e0-20230809.5 -n acstor ",
  "kubectl set image deployment/acstor-csi-controller csi-controller=azstortest.azurecr.io/artifact/424bd44c-13b4-4637-a5a4-0b9506e90413/buddy/csi.controller:b7497942a0b4bfaa2f4467ff9132dbb24d110790-20230809.1 -n acstor",
  "kubectl set image daemonset/acstor-io-engine io-engine=azstortest.azurecr.io/artifact/424bd44c-13b4-4637-a5a4-0b9506e90413/buddy/mayastor-io-engine:6b2a0d7946981ffccefee65698c9f5cb57c19d62-20230801.1 -n acstor"
]
|> Enum.each(fn each_cmd -> each_cmd |> ExecCmd.run() end)
```

<!-- livebook:{"branch_parent_index":0} -->

## 5. Create Storage Pool

Create storage pools

* For Azure disk, create at least 3 storage pools.
* For NVMe, just create 1 storage pool.
* For SAN, I haven't tried yet, I guess also at least 3 storage pools are needed.

**Notice**: please check the `disk_type` value from section 1.

```elixir
disk_type = "azure_disk"

num_storage_pool =
  case disk_type do
    "nvme" -> 1
    "azure_disk" -> 3
    "san" -> 3
  end
```

```elixir
# generate different yml files for different storage type
storage_pool_yml_files =
  case disk_type do
    "azure_disk" ->
      1..num_storage_pool
      |> Enum.to_list()
      |> Enum.map(fn index ->
        each_storage_pool_yml = "~/download/azure_disk_storage_pool_#{index}.yaml"

        """
        cat<<EOF >#{each_storage_pool_yml}
        apiVersion: containerstorage.azure.com/v1alpha1 
        kind: StoragePool 
        metadata: 
          name: manageddisk#{index} 
          namespace: acstor 
        spec: 
          resources: 
            requests: 
              storage: 10Ti 
          poolType: 
            azureDisk: {} 
        EOF
        """
        |> ExecCmd.run()

        # reture each yml file 
        each_storage_pool_yml
      end)

    "nvme" ->
      1..num_storage_pool
      |> Enum.to_list()
      |> Enum.map(fn index ->
        each_storage_pool_yml = "~/download/nvme_storage_pool_#{index}.yaml"

        """
        cat<<EOF >#{each_storage_pool_yml}
        apiVersion: containerstorage.azure.com/v1alpha1 
        kind: StoragePool 
        metadata: 
          name: ephemeraldisk#{index}  
          namespace: acstor 
        spec: 
          poolType: 
            ephemeralDisk: {} 
        EOF
        """
        |> ExecCmd.run()

        # reture each yml file 
        each_storage_pool_yml
      end)

    "san" ->
      1..num_storage_pool
      |> Enum.to_list()
      |> Enum.map(fn index ->
        each_storage_pool_yml = "~/download/san_storage_pool_#{index}.yaml"

        """
        cat<<EOF >#{each_storage_pool_yml}
        apiVersion: containerstorage.azure.com/v1alpha1 
        kind: StoragePool 
        metadata: 
          name: acstor-managed#{index} 
          namespace: acstor 
        spec: 
          poolType: 
            san: {} 
        resources: 
          limits: {"storage": 15Ti} 
          requests: {"storage": 1Ti}  
        EOF
        """
        |> ExecCmd.run()

        # reture each yml file 
        each_storage_pool_yml
      end)
  end
```

```elixir
# Create storage pool from storage_pool_yml_files
storage_pool_yml_files
|> Enum.each(fn each_yaml ->
  "kubectl apply -f  #{each_yaml}" |> ExecCmd.run()
end)
```

<!-- livebook:{"branch_parent_index":0} -->

## 6. Create Storage Class

```elixir
storage_class_yaml = "~/download/storage_class.yaml"

"""
cat<<EOF >#{storage_class_yaml}
apiVersion: storage.k8s.io/v1 
kind: StorageClass 
metadata: 
  name: acstor-replication 
parameters: 
  ioTimeout: "60" 
  protocol: nvmf 
  repl: "3" 
  thin: "true" 
provisioner: containerstorage.csi.azure.com 
reclaimPolicy: Delete 
volumeBindingMode: WaitForFirstConsumer 
EOF
"""
|> ExecCmd.run()
```

```elixir
"kubectl apply -f  #{storage_class_yaml}" |> ExecCmd.run()
```

## 7. Create PVC

Create PVC(s) with storage class `acstor-replication`
Change the `name` and `storage` (size) as needed

```elixir
pvc_name = "pv-00-000"
storage_size = "100Gi"
```

```elixir
pvc_yaml = "~/download/pvc.yaml"

"""
cat<<EOF >#{pvc_yaml}
apiVersion: v1 
kind: PersistentVolumeClaim 
metadata: 
  name: #{pvc_name}
spec: 
  accessModes: 
  - ReadWriteOnce 
  resources: 
    requests: 
      storage: #{storage_size} 
  storageClassName: acstor-replication  
EOF
"""
|> ExecCmd.run()
```

```elixir
"kubectl apply -f  #{pvc_yaml}" |> ExecCmd.run()
```

```elixir
# Check created PVC 
"kubectl get pvc -o wide"
|> ExecCmd.run()
```

After creation, the pvc is in `pending` status. We need to create a pod to consume it.

## 8. Create Pod

To create pod on specific node, we need to label those node first.

```elixir
# 1. get the node names related with "storagepool"
"kubectl get nodes | grep storagepool"
|> ExecCmd.run()
```

```elixir
# 2. Label the nodes "targetNode=xxx"
[
  "aks-storagepool-17429370-vmss000000",
  "aks-storagepool-17429370-vmss000001",
  "aks-storagepool-17429370-vmss000002"
]
|> Enum.with_index()
|> Enum.each(fn {each_node, i} ->
  "kubectl label nodes #{each_node} targetNode=node#{i}"
  |> ExecCmd.run()
end)
```

```elixir
# Check labeled node 
"kubectl get node --show-labels | grep targetNode"
|> ExecCmd.run()
```

```elixir
# Firnall, create pod on specific node
# from previous steps in this section
target_node = "node0"
# custom
pod_name = "test-pod-00-000"
# from section 7.
pvc_name = "pv-00-000"

pod_yaml = "~/download/pod.yaml"

"""
cat<<EOF >#{pod_yaml}
kind: Pod 
apiVersion: v1 
metadata: 
  name: #{pod_name} 
spec: 
  volumes: 
    - name: ms-volume 
      persistentVolumeClaim: 
        claimName: #{pvc_name} 
  containers: 
    - name: #{pod_name} 
      image: acrldi.azurecr.io/acstor-test 
      args: 
        - sleep 
        - "1000000" 
      volumeMounts: 
        - mountPath: "/volume" 
          name: ms-volume 
  nodeSelector: 
    targetNode: "#{target_node}"   
EOF
"""
|> ExecCmd.run()
```

```elixir
"kubectl apply -f  #{pod_yaml}" |> ExecCmd.run()
```

```elixir
# check pod to make sure it is in Running status
"kubectl get pods -o wide"
|> ExecCmd.run()
```

## 9. Run Fio workload

```elixir
"""
kubectl exec -it #{pod_name} -- fio \
--name=benchtest --size=2g \
--filename=/volume/test \
--direct=1 --rw=randrw --rwmixread=30 \
--ioengine=libaio --bs=4k --iodepth=8 \
--numjobs=1 --time_based \
--runtime=60 \
--verify_backlog=4096 \
--serialize_overlap=1 \
--do_verify=1 \
--verify=crc32 --group_reporting
"""
|> ExecCmd.run()
```

For each node, there is a corresponding io-engine pod.

```elixir
# After the workload finished, we could check the replication
"kubectl get pod -n acstor | grep io-engine " |> ExecCmd.run()
```

We need to verify the md5's value are the same across those io-engine. \
Just check them one by one for each io-engine from real terminal because we need to login to it to calculate the md5 values for each volumn.

```
$ kubectl exec -it acstor-io-engine-6dnql -n acstor -c io-engine -- sh
sh-5.1# ls -lht /xfs-disk-pool/csi-7zjkf/be3adf14-e030-438a-9067-342366581c97
-rw-r--r-- 1 root root 100G Aug 24 08:24 /xfs-disk-pool/csi-7zjkf/be3adf14-e030-438a-9067-342366581c97
sh-5.1# ls -lht /xfs-disk-pool/csi-7zjkf/
total 4.1G
-rw-r--r-- 1 root root 100G Aug 24 08:24 be3adf14-e030-438a-9067-342366581c97
sh-5.1# md5sum /xfs-disk-pool/csi-7zjkf/*
65bc81823676e57a871278e08574e470  /xfs-disk-pool/csi-7zjkf/be3adf14-e030-438a-9067-342366581c97
```

<!-- livebook:{"break_markdown":true} -->

The purpose of this is make sure all md5 values of each volumes for different io-engines are all the same.

## 10. Replication Rebuild

So far, within a AKS cluster

* Created multiple nodes

* Label those nodes with tag: `acstor.azure.com/io-engine=acstor`

* Those labeled nodes will make acstor extension to create special pods checked by:

  ```
    kubectl get pod -n acstor | grep io-engine 
  ```

  * Each io-engine is a running application in pod with special name.

  That's why we need to first add node pool and label the nodes before we install acstor-extension.

<!-- livebook:{"break_markdown":true} -->

What we need to test in replication rebuild is:

* Remove the `acstor.azure.com/io-engine=acstor` from some node. 
  The purpose of this is to simualte incident in io-engine.
* Then, re-add the label back to the node.
* At last, we check the rebuilding 
  * Rebuilding means the volumes in some io-engine, such as `acstor-io-engine-6dnql` should be recovered.

<!-- livebook:{"break_markdown":true} -->

### Rebuilding steps

<!-- livebook:{"break_markdown":true} -->

1. Delete io-engine pod by unlabel one of the node.

**Notice**

* We need to unable the node which there is no user pod created on it.
* For instance, the fio application is created on node0, we should **NOT**  unlable this node.

<!-- livebook:{"break_markdown":true} -->

1. Verify the one of the io-engine is removed 
   ```
   kubectl get pod -n acstor | grep io-engine
   ```

<!-- livebook:{"break_markdown":true} -->

1. Add label back `acstor.azure.com/io-engine=acstor` to the node.

**Notice**

* Based on the interval of time between relabel, there is `Rebuilding` and `Partial Rebuilding`
* This doesn't afffect how to test rebuilding or how to verify its result. But it has differences

<!-- livebook:{"break_markdown":true} -->

1. Wait rebulding finished then check MD5

<!-- livebook:{"break_markdown":true} -->

1. Make sure after rebuilding the diskpools' size should be almost same
   ```
   zw@zwpdbh:~/download/azstore-add-ons$ kubectl get diskpool -A
   NAMESPACE   NAME                          CAPACITY         AVAILABLE        USED          RESERVED       READY   AGE
   acstor      manageddisk1-diskpool-mwoar   10992979238912   10911932907520   81046331392   184052908032   True    87m
   acstor      manageddisk2-diskpool-fwrfj   10992979238912   10911932907520   81046331392   184052908032   True    87m
   acstor      manageddisk3-diskpool-nqcwv   10992979238912   10911932907520   81046331392   184052908032   True    87m
   ```

<!-- livebook:{"break_markdown":true} -->

### Rebuilding VS Partial Rebuilding

Rebuilding

* Rebuilding on a single io-engine
* Rebuilding on multiple (e.g. 2) io-engines in parallel (a cluster with 4 or more nodes is needed)
* Bring down all nodes except the nexus node, then bring them up, delete pods/PVs, create pods/PVs

Partial rebuilding

* Partial rebuilding on a single io-engine
* Partial rebuilding on a single io-engine for multiple times
* Partial rebuilding on multiple (e.g. 2) io-engines in parallel (a cluster with 4 or more nodes is needed)

<!-- livebook:{"break_markdown":true} -->

Other notes:

* The finish of rebuilding is indicated by `state: Online`.

<!-- livebook:{"branch_parent_index":0} -->

## Troubleshooting: helm install acstor-addon error

### Troubleshooting

Error: INSTALLATION FAILED: failed post-install: job failed: BackoffLimitExceeded

Solution:

* remove installed helm extension
* remove acstor namespace

```elixir
# To make sure extension status
"helm list -n acstor"
|> ExecCmd.run()
```

```elixir
# To uninstall failed extension
"helm uninstall acstor -n acstor"
|> ExecCmd.run()
```

```elixir
# Check if there is "acstor" namespace
"kubectl get namespace -A "
|> ExecCmd.run()
```

```elixir
# Delete "acstor" namespace
"kubectl delete namespace acstor "
|> ExecCmd.run()
```

```elixir
# Make sure there is no pod in acstor namespace
"kubectl get pods -n acstor"
|> ExecCmd.run()
```
