# ACStor Replication

## Introduction

### References

* [Steps of replication](https://microsoft.sharepoint.com/teams/AzureStorage/Scenarios/_layouts/15/Doc.aspx?sourcedoc={2fa64dce-8dcf-4568-8d77-2c3c6b5516bf}&action=edit&wd=target%28Storage%20Features%2FGeneral%2FContainer%20and%20AKS.one%7Cfea4ea3b-64a2-4629-a7ad-c7003b84b71f%2FSteps%7C71d3e2c7-541a-4a17-83c4-275279f3ee3c%2F%29&wdorigin=NavigationUrl)
* [How to rebuild](https://microsoft.sharepoint.com/teams/AzureStorage/Scenarios/_layouts/15/Doc.aspx?sourcedoc={2fa64dce-8dcf-4568-8d77-2c3c6b5516bf}&action=edit&wd=target%28Storage%20Features%2FGeneral%2FContainer%20and%20AKS.one%7Cfea4ea3b-64a2-4629-a7ad-c7003b84b71f%2FRebuild%7C531e371e-20dd-4216-bb10-65f9a5638bda%2F%29&wdorigin=NavigationUrl)

<!-- livebook:{"branch_parent_index":0} -->

## 1. Create AKS Cluster

```elixir
# Our global settings 
# subscription has no particular
subscription = "65490f91-f2c2-4514-80ba-4ec1de89aeda"
# region has no particular
region = "eastus"
```

```elixir
# For our repliation test  
disk_type = "azure_disk"
rg = "acstor-replication-test"
aks_cluster = "acstor-replication-test01"
```

```elixir
# Set Azure subscription
ExecCmd.run("az account set --subscription #{subscription}")
```

```elixir
# Create resource group on Azure
ExecCmd.run("az group create  --location #{region} --name #{rg}")
```

```elixir
# Create AKS cluster
ExecCmd.run(
  "az aks create -n #{aks_cluster} -g #{rg} --generate-ssh-keys --attach-acr /subscriptions/d64ddb0c-7399-4529-a2b6-037b33265372/resourceGroups/azstor-test-rg/providers/Microsoft.ContainerRegistry/registries/azstortest"
)
```

```elixir
# Prepare the VM size for node pool
vm_sku =
  case disk_type do
    "azure_disk" -> "Standard_D4s_v3"
    "nvme" -> "Standard_L8s_v3"
  end
```

```elixir
# Add node pool with 3 nodes 
"az aks nodepool add --cluster-name #{aks_cluster} --name storagepool --resource-group #{rg} --node-vm-size #{vm_sku} --node-count 3 "
|> ExecCmd.run()
```

So far, the created AKS cluster is

```elixir
%{
  subscription_id: subscription,
  region: region,
  rg: rg,
  aks: aks_cluster
}
```

<!-- livebook:{"branch_parent_index":0} -->

## 2. Configure AKS Cluster

To execute kubectl command, we need to get the AKS context by overwrite local kubectl config file.

```elixir
aks_settings = %{
  aks: "acstor-replication-test01",
  region: "eastus",
  rg: "acstor-replication-test",
  subscription_id: "65490f91-f2c2-4514-80ba-4ec1de89aeda"
}
```

```elixir
alias Azure.Aks
k8s_config = Aks.get_aks_config(aks_settings)
```

```elixir
File.write("/home/zw/.kube/config", k8s_config)
```

```elixir
# Test k8s context by checking the nodes in AKS cluster
ExecCmd.run("kubectl get nodes")
```

```elixir
# Label the nodes 
"kubectl label nodes --selector agentpool=storagepool acstor.azure.com/io-engine=acstor"
|> ExecCmd.run()
```

```elixir
# show node label 
"kubectl get nodes --show-labels"
|> ExecCmd.run()
```

```elixir
# to remove lablels "acstor.azure.com/io-engine=acstor"
# from node with label = "agentpool=nodepool1" 
"kubectl label nodes --selector agentpool=nodepool1 acstor.azure.com/io-engine-"
|> ExecCmd.run()
```

### Assign contributor role

```elixir
# Check the managed id 
"az aks show -g #{aks_settings.rg} -n #{aks_settings.aks} --out tsv --query identityProfile.kubeletidentity.objectId"
|> ExecCmd.run()
```

```elixir
# check the node resource group 
"az aks show -g #{aks_settings.rg} -n #{aks_settings.aks} --out tsv --query nodeResourceGroup"
|> ExecCmd.run()
```

```elixir
# Obtained from previous 2 steps as ExecCmd's output to terminal
managed_id = "c84af941-c89e-4256-b871-861449a8fc39"
node_rg = "MC_acstor-replication-test_acstor-replication-test01_eastus"
```

```elixir
# Finally, assign the contributor role
"az role assignment create --assignee #{managed_id} --role Contributor --scope /subscriptions/#{aks_settings.subscription_id}/resourceGroups/#{node_rg}"
|> ExecCmd.run()
```

<!-- livebook:{"branch_parent_index":0} -->

## 3. Install ACStor Addons

[azstor-add-ons](https://dev.azure.com/msazure/One/_git/azstor-add-ons) hosts the main Helm chart for ACStor and additional components added by Microsoft.

First, let's check the prerequisites: 
AKS nodes where ACStor should run must be labeled with `acstor.azure.com/io-engine=acstor`.

Notice: the kubectl context is inherited from previous section

<!-- livebook:{"break_markdown":true} -->

###

<!-- livebook:{"break_markdown":true} -->

We need to download the latest add-ons and install it from source code:

```elixir
download_folder = Path.join(["~/download", "azstore-add-ons"])
```

```elixir
"git clone git@ssh.dev.azure.com:v3/msazure/One/azstor-add-ons #{download_folder}"
|> ExecCmd.run()
```

```elixir
# Build add-on
"""
cd #{download_folder}/charts/latest/ && 
helm dependency build
"""
|> ExecCmd.run()
```

```elixir

```

```elixir
"""
cd #{download_folder} && 
helm install acstor charts/latest --namespace acstor --create-namespace \
--version 0.0.0-latest \
--set image.tag=latest \
--set image.registry="azstortest.azurecr.io" \
--set image.repo="mayadata" \
--set capacityProvisioner.image.tag=latest \
--set capacityProvisioner.image.registry="azstortest.azurecr.io"
"""
|> ExecCmd.run()
```

If installation succeed, it should show messages like:

```
AME: acstor
LAST DEPLOYED: Thu Aug 24 14:31:45 2023
NAMESPACE: acstor
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
ACStor has been installed. Check its status by running:
$ kubectl get pods -n acstor
```

So, let check it

```elixir
"kubectl get pods -n acstor"
|> ExecCmd.run()
```

<!-- livebook:{"branch_parent_index":0} -->

## 4. Configure Replication

Notice: the kubectl context is inherited from previous section 2.

<!-- livebook:{"break_markdown":true} -->

Patch the replication images

```elixir
# All these are hardcoded values
# Where are those values come from ？？？
[
  "kubectl set image deployment/acstor-api-rest api-rest=azstortest.azurecr.io/artifact/424bd44c-13b4-4637-a5a4-0b9506e90413/buddy/rest:47c414cef91d05651985c66d6f3bbe317aab35e0-20230809.5 -n acstor",
  "kubectl set image deployment/acstor-agent-core agent-core=azstortest.azurecr.io/artifact/424bd44c-13b4-4637-a5a4-0b9506e90413/buddy/agents.core:47c414cef91d05651985c66d6f3bbe317aab35e0-20230809.5 -n acstor ",
  "kubectl set image deployment/acstor-csi-controller csi-controller=azstortest.azurecr.io/artifact/424bd44c-13b4-4637-a5a4-0b9506e90413/buddy/csi.controller:b7497942a0b4bfaa2f4467ff9132dbb24d110790-20230809.1 -n acstor",
  "kubectl set image daemonset/acstor-io-engine io-engine=azstortest.azurecr.io/artifact/424bd44c-13b4-4637-a5a4-0b9506e90413/buddy/mayastor-io-engine:6b2a0d7946981ffccefee65698c9f5cb57c19d62-20230801.1 -n acstor"
]
|> Enum.each(fn each_cmd -> each_cmd |> ExecCmd.run() end)
```

<!-- livebook:{"branch_parent_index":0} -->

## 5. Create Storage Pool

Create storage pools

* For Azure disk, create at least 3 storage pools.
* For NVMe, just create 1 storage pool.
* For SAN, I haven't tried yet, I guess also at least 3 storage pools are needed.

**Notice**: please check the `disk_type` value from section 1.

```elixir
disk_type = "azure_disk"

num_storage_pool =
  case disk_type do
    "nvme" -> 1
    "azure_disk" -> 3
    "san" -> 3
  end
```

```elixir
# generate different yml files for different storage type
storage_pool_yml_files =
  case disk_type do
    "azure_disk" ->
      1..num_storage_pool
      |> Enum.to_list()
      |> Enum.map(fn index ->
        each_storage_pool_yml = "~/download/azure_disk_storage_pool_#{index}.yaml"

        """
        cat<<EOF >#{each_storage_pool_yml}
        apiVersion: containerstorage.azure.com/v1alpha1 
        kind: StoragePool 
        metadata: 
          name: manageddisk#{index} 
          namespace: acstor 
        spec: 
          resources: 
            requests: 
              storage: 10Ti 
          poolType: 
            azureDisk: {} 
        EOF
        """
        |> ExecCmd.run()

        # reture each yml file 
        each_storage_pool_yml
      end)

    "nvme" ->
      1..num_storage_pool
      |> Enum.to_list()
      |> Enum.map(fn index ->
        each_storage_pool_yml = "~/download/nvme_storage_pool_#{index}.yaml"

        """
        cat<<EOF >#{each_storage_pool_yml}
        apiVersion: containerstorage.azure.com/v1alpha1 
        kind: StoragePool 
        metadata: 
          name: ephemeraldisk#{index}  
          namespace: acstor 
        spec: 
          poolType: 
            ephemeralDisk: {} 
        EOF
        """
        |> ExecCmd.run()

        # reture each yml file 
        each_storage_pool_yml
      end)

    "san" ->
      1..num_storage_pool
      |> Enum.to_list()
      |> Enum.map(fn index ->
        each_storage_pool_yml = "~/download/san_storage_pool_#{index}.yaml"

        """
        cat<<EOF >#{each_storage_pool_yml}
        apiVersion: containerstorage.azure.com/v1alpha1 
        kind: StoragePool 
        metadata: 
          name: acstor-managed#{index} 
          namespace: acstor 
        spec: 
          poolType: 
            san: {} 
        resources: 
          limits: {"storage": 15Ti} 
          requests: {"storage": 1Ti}  
        EOF
        """
        |> ExecCmd.run()

        # reture each yml file 
        each_storage_pool_yml
      end)
  end
```

```elixir
# Create storage pool from storage_pool_yml_files
storage_pool_yml_files
|> Enum.each(fn each_yaml ->
  "kubectl apply -f  #{each_yaml}" |> ExecCmd.run()
end)
```

<!-- livebook:{"branch_parent_index":0} -->

## 6. Create Storage Class

```elixir
storage_class_yaml = "~/download/storage_class.yaml"

"""
cat<<EOF >#{storage_class_yaml}
apiVersion: storage.k8s.io/v1 
kind: StorageClass 
metadata: 
  name: acstor-replication 
parameters: 
  ioTimeout: "60" 
  protocol: nvmf 
  repl: "3" 
  thin: "true" 
provisioner: containerstorage.csi.azure.com 
reclaimPolicy: Delete 
volumeBindingMode: WaitForFirstConsumer 
EOF
"""
|> ExecCmd.run()
```

```elixir
"kubectl apply -f  #{storage_class_yaml}" |> ExecCmd.run()
```

<!-- livebook:{"branch_parent_index":0} -->

## 7. Create PVC

Create PVC(s) with storage class `acstor-replication`
Change the `name` and `storage` (size) as needed

```elixir
pvc_settings = %{
  name: "pv-00-001",
  size: "100Gi"
}
```

```elixir
pvc_yaml = "~/download/pvc.yaml"

"""
cat<<EOF >#{pvc_yaml}
apiVersion: v1 
kind: PersistentVolumeClaim 
metadata: 
  name: #{pvc_settings.name}
spec: 
  accessModes: 
  - ReadWriteOnce 
  resources: 
    requests: 
      storage: #{pvc_settings.size} 
  storageClassName: acstor-replication  
EOF
"""
|> ExecCmd.run()
```

```elixir
"kubectl apply -f  #{pvc_yaml}" |> ExecCmd.run()
```

```elixir
# Check created PVC 
"kubectl get pvc -o wide"
|> ExecCmd.run()
```

After creation, the pvc is in `pending` status. We need to create a pod to consume it.

## 8. Create Pod

### Label the nodes

To create pod on specific node, we need to label those node first.

```elixir
# 1. get the node names related with "storagepool"
"kubectl get nodes | grep storagepool"
|> ExecCmd.run()
```

```elixir
# 2. Label the nodes "targetNode=xxx"
[
  "aks-storagepool-17429370-vmss000000",
  "aks-storagepool-17429370-vmss000001",
  "aks-storagepool-17429370-vmss000002"
]
|> Enum.with_index()
|> Enum.each(fn {each_node, i} ->
  "kubectl label nodes #{each_node} targetNode=node#{i}"
  |> ExecCmd.run()
end)
```

```elixir
# Check labeled node 
"kubectl get node --show-labels | grep targetNode"
|> ExecCmd.run()
```

### Create pod on specific node

```elixir
pod_settings = %{
  node_label: "node0",
  pvc: pvc_settings.name,
  name: "test-pod-00-002",
  yaml: "~/download/pod.yaml"
}
```

```elixir
# Firnall, create pod on specific node
"""
cat<<EOF >#{pod_settings.yaml}
kind: Pod 
apiVersion: v1 
metadata: 
  name: #{pod_settings.name} 
spec: 
  volumes: 
    - name: ms-volume 
      persistentVolumeClaim: 
        claimName: #{pod_settings.pvc} 
  containers: 
    - name: #{pod_settings.name} 
      image: acrldi.azurecr.io/acstor-test 
      args: 
        - sleep 
        - "1000000" 
      volumeMounts: 
        - mountPath: "/volume" 
          name: ms-volume 
  nodeSelector: 
    targetNode: "#{pod_settings.node_label}"   
EOF
"""
|> ExecCmd.run()
```

```elixir
"kubectl apply -f  #{pod_settings.yaml}" |> ExecCmd.run()
```

```elixir
# check pod to make sure it is in Running status
"kubectl get pods -o wide"
|> ExecCmd.run()
```

## 9. Run Fio workload

```elixir
"""
kubectl exec -it #{pod_settings.name} -- fio \
--name=benchtest --size=2g \
--filename=/volume/test \
--direct=1 --rw=randrw --rwmixread=30 \
--ioengine=libaio --bs=4k --iodepth=8 \
--numjobs=1 --time_based \
--runtime=60 \
--verify_backlog=4096 \
--serialize_overlap=1 \
--do_verify=1 \
--verify=crc32 --group_reporting
"""
|> ExecCmd.run()
```

For each node, there is a corresponding io-engine pod.

```elixir
# After the workload finished, we could check the replication
"kubectl get pod -n acstor | grep io-engine " |> ExecCmd.run()
```

We need to verify the md5's value are the same across those io-engine. \
Just check them one by one for each io-engine from real terminal because we need to login to it to calculate the md5 values for each volumn.

```
$ kubectl exec -it acstor-io-engine-6dnql -n acstor -c io-engine -- sh
sh-5.1# ls -lht /xfs-disk-pool/csi-7zjkf/be3adf14-e030-438a-9067-342366581c97
-rw-r--r-- 1 root root 100G Aug 24 08:24 /xfs-disk-pool/csi-7zjkf/be3adf14-e030-438a-9067-342366581c97
sh-5.1# ls -lht /xfs-disk-pool/csi-7zjkf/
total 4.1G
-rw-r--r-- 1 root root 100G Aug 24 08:24 be3adf14-e030-438a-9067-342366581c97
sh-5.1# md5sum /xfs-disk-pool/csi-7zjkf/*
65bc81823676e57a871278e08574e470  /xfs-disk-pool/csi-7zjkf/be3adf14-e030-438a-9067-342366581c97
```

<!-- livebook:{"break_markdown":true} -->

* The purpose of this is make sure all md5 values of each volumes for different io-engines are all the same.

<!-- livebook:{"break_markdown":true} -->

The current problem for checking or running md5 on io-engine's file is we don't know its name beforehand. \
How to know this value in advance?

```elixir
"kubectl exec -it acstor-io-engine-6dnql -n acstor -c io-engine -- ls -lht /xfs-disk-pool/"
|> ExecCmd.run()
```

<!-- livebook:{"branch_parent_index":0} -->

## 10. Replication Rebuild

### Roadmap

So far, within a AKS cluster

* Created multiple nodes

* Label those nodes with tag: `acstor.azure.com/io-engine=acstor`

* Those labeled nodes will make acstor extension to create special pods:

  ```
    $kubectl get pod -n acstor | grep io-engine 
    acstor-io-engine-6dnql                                 2/2     Running                 0              83m
    acstor-io-engine-dg9gm                                 2/2     Running                 0              84m
    acstor-io-engine-v8drc                                 2/2     Running                 0              86m 
  ```

  Each io-engine is a running application in pod with special name.\
  That's why we need to first add node pool and label the nodes before we install acstor-extension.

<!-- livebook:{"break_markdown":true} -->

What we need to test in replication rebuild is:

* Remove the `acstor.azure.com/io-engine=acstor` from some node. 
  The purpose of this is to simualte incident in io-engine.
* Then, re-add the label back to the node.
* At last, we check the rebuilding 
  * Rebuilding means the volumes(PVC) in some io-engine, such as `acstor-io-engine-6dnql` should be recovered.
  * Currently, we just created one PVC. There could be multiple PVs for one node.

<!-- livebook:{"break_markdown":true} -->

### Rebuilding steps

<!-- livebook:{"break_markdown":true} -->

1. Delete pod with label "io-engine" by unlabelling it.

**Notice**

* We need to unable the node which there is no user pod created on it.
* For instance, the fio application is created on node0, we should **NOT**  unlable this node.
* For rebuilding work, the minimum labeled node must be `>=` 2.

```elixir
# Check node with "io-engine" label
"kubectl get node --show-labels | grep io-engine" |> ExecCmd.run()
```

Remember, when we create pod, we create it on specific node. This is done by (see section 8):

* Label each node
* Specify the node's label in the pod's yaml file.

```elixir
# From section 8
pod_settings = %{
  name: "test-pod-00-002",
  node_label: "node0",
  pvc: "pv-00-001",
  yaml: "~/download/pod.yaml"
}
```

So, we need to check the current running pods to see which nodes they are running on.

```elixir
"kubectl get pods -o wide"
|> ExecCmd.run()
```

So, from the output:

```
10:53:29.683 [info] kubectl get node --show-labels | grep io-engine
aks-storagepool-17429370-vmss000000   Ready    agent   23h   v1.26.6   acstor.azure.com/io-engine=acstor,agentpool=storagepool,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=Standard_D4s_v3,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=eastus,failure-domain.beta.kubernetes.io/zone=0,kubernetes.azure.com/agentpool=storagepool,kubernetes.azure.com/cluster=MC_acstor-replication-test_acstor-replication-test01_eastus,kubernetes.azure.com/consolidated-additional-properties=dc28639d-422c-11ee-8ff3-aefbca123c25,kubernetes.azure.com/kubelet-identity-client-id=e3681fa3-21a4-44ba-9cbb-e1b9c1f57ac0,kubernetes.azure.com/mode=user,kubernetes.azure.com/node-image-version=AKSUbuntu-2204gen2containerd-202308.10.0,kubernetes.azure.com/nodepool-type=VirtualMachineScaleSets,kubernetes.azure.com/os-sku=Ubuntu,kubernetes.azure.com/role=agent,kubernetes.azure.com/storageprofile=managed,kubernetes.azure.com/storagetier=Premium_LRS,kubernetes.io/arch=amd64,kubernetes.io/hostname=aks-storagepool-17429370-vmss000000,kubernetes.io/os=linux,kubernetes.io/role=agent,node-role.kubernetes.io/agent=,node.kubernetes.io/instance-type=Standard_D4s_v3,openebs.io/nodename=aks-storagepool-17429370-vmss000000,storageprofile=managed,storagetier=Premium_LRS,targetNode=node0,topology.disk.csi.azure.com/zone=,topology.kubernetes.io/region=eastus,topology.kubernetes.io/zone=0,topology.san.csi.azure.com/zone=
aks-storagepool-17429370-vmss000001   Ready    agent   23h   v1.26.6   acstor.azure.com/io-engine=acstor,agentpool=storagepool,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=Standard_D4s_v3,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=eastus,failure-domain.beta.kubernetes.io/zone=0,kubernetes.azure.com/agentpool=storagepool,kubernetes.azure.com/cluster=MC_acstor-replication-test_acstor-replication-test01_eastus,kubernetes.azure.com/consolidated-additional-properties=dc28639d-422c-11ee-8ff3-aefbca123c25,kubernetes.azure.com/kubelet-identity-client-id=e3681fa3-21a4-44ba-9cbb-e1b9c1f57ac0,kubernetes.azure.com/mode=user,kubernetes.azure.com/node-image-version=AKSUbuntu-2204gen2containerd-202308.10.0,kubernetes.azure.com/nodepool-type=VirtualMachineScaleSets,kubernetes.azure.com/os-sku=Ubuntu,kubernetes.azure.com/role=agent,kubernetes.azure.com/storageprofile=managed,kubernetes.azure.com/storagetier=Premium_LRS,kubernetes.io/arch=amd64,kubernetes.io/hostname=aks-storagepool-17429370-vmss000001,kubernetes.io/os=linux,kubernetes.io/role=agent,node-role.kubernetes.io/agent=,node.kubernetes.io/instance-type=Standard_D4s_v3,openebs.io/nodename=aks-storagepool-17429370-vmss000001,storageprofile=managed,storagetier=Premium_LRS,targetNode=node1,topology.disk.csi.azure.com/zone=,topology.kubernetes.io/region=eastus,topology.kubernetes.io/zone=0,topology.san.csi.azure.com/zone=
aks-storagepool-17429370-vmss000002   Ready    agent   23h   v1.26.6   acstor.azure.com/io-engine=acstor,agentpool=storagepool,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=Standard_D4s_v3,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=eastus,failure-domain.beta.kubernetes.io/zone=0,kubernetes.azure.com/agentpool=storagepool,kubernetes.azure.com/cluster=MC_acstor-replication-test_acstor-replication-test01_eastus,kubernetes.azure.com/consolidated-additional-properties=dc28639d-422c-11ee-8ff3-aefbca123c25,kubernetes.azure.com/kubelet-identity-client-id=e3681fa3-21a4-44ba-9cbb-e1b9c1f57ac0,kubernetes.azure.com/mode=user,kubernetes.azure.com/node-image-version=AKSUbuntu-2204gen2containerd-202308.10.0,kubernetes.azure.com/nodepool-type=VirtualMachineScaleSets,kubernetes.azure.com/os-sku=Ubuntu,kubernetes.azure.com/role=agent,kubernetes.azure.com/storageprofile=managed,kubernetes.azure.com/storagetier=Premium_LRS,kubernetes.io/arch=amd64,kubernetes.io/hostname=aks-storagepool-17429370-vmss000002,kubernetes.io/os=linux,kubernetes.io/role=agent,node-role.kubernetes.io/agent=,node.kubernetes.io/instance-type=Standard_D4s_v3,openebs.io/nodename=aks-storagepool-17429370-vmss000002,storageprofile=managed,storagetier=Premium_LRS,targetNode=node2,topology.disk.csi.azure.com/zone=,topology.kubernetes.io/region=eastus,topology.kubernetes.io/zone=0,topology.san.csi.azure.com/zone=

```

and

```
10:53:40.546 [info] kubectl get pods -o wide
NAME              READY   STATUS    RESTARTS   AGE   IP            NODE                                  NOMINATED NODE   READINESS GATES
test-pod-00-000   1/1     Running   0          18h   10.244.3.24   aks-storagepool-17429370-vmss000000   <none>           <none>
test-pod-00-002   1/1     Running   0          34m   10.244.3.25   aks-storagepool-17429370-vmss000000   <none>           <none>
```

we can unlabel "aks-storagepool-17429370-vmss000001" or "aks-storagepool-17429370-vmss000002".

```elixir
"kubectl label node aks-storagepool-17429370-vmss000001 acstor.azure.com/io-engine-"
|> ExecCmd.run()
```

1. Verify the one of the io-engine is removed

```elixir
"kubectl get node --show-labels | grep io-engine" |> ExecCmd.run()
```

Each labelled io-engine node will have a corresponding io-engine pod in namespace `acstor`. When we remove the label from a node, the corresponding pod is also removed.

```elixir
"kubectl get pod -n acstor | grep io-engine"
|> ExecCmd.run()
```

1. Add label back `acstor.azure.com/io-engine=acstor` to the node.

**Notice**

* Based on the interval of time between relabel, there is `Rebuilding` and `Partial Rebuilding`
* This doesn't afffect how to test rebuilding or how to verify its result. But it has differences

```elixir
"kubectl label node aks-storagepool-17429370-vmss000001 acstor.azure.com/io-engine=acstor"
|> ExecCmd.run()
```

```elixir
"kubectl get node --show-labels | grep io-engine"
|> ExecCmd.run()
```

```elixir
# Verify the corresponding pod in acstor namespace are recovered
"kubectl get pod -n acstor | grep io-engine"
|> ExecCmd.run()
```

Verify the rebuilding is finished.

```elixir
"kubectl get pod -n acstor | grep api-rest"
|> ExecCmd.run()
```

```elixir
# The "&" make the command execute in background
host_port = 9096

port_mapping_pid =
  Task.start(fn ->
    "kubectl port-forward acstor-api-rest-7f6f545689-fnchn -n acstor #{host_port}:8081"
    |> ExecCmd.run()
  end)
```

```elixir
"curl -X 'GET' 'http://127.0.0.1:#{host_port}/v0/volumes?max_entries=1' -H 'accept: application/json'  | jq"
|> ExecCmd.run()
```

The output

```
$ curl -X 'GET' 'http://127.0.0.1:9092/v0/volumes?max_entries=1' -H 'accept: application/json'  | jq
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0Handling connection for 9092
100  1626  100  1626    0     0   1818      0 --:--:-- --:--:-- --:--:--  1816
{
  "entries": [
    {
      "spec": {
        "num_replicas": 3,
        "size": 107374182400,
        "status": "Created",
        "target": {
          "node": "aks-storagepool-17429370-vmss000000",
          "protocol": "nvmf"
        },
        "uuid": "3e675149-2633-421c-b090-c6e4801a7f56",
        "topology": {
          "pool_topology": {
            "labelled": {
              "exclusion": {},
              "inclusion": {
                "openebs.io/created-by": "operator-diskpool"
              }
            }
          }
        },
        "policy": {
          "self_heal": true
        },
        "thin": true
      },
      "state": {
        "target": {
          "children": [
            {
              "state": "Online",
              "uri": "bdev:////xfs-disk-pool/csi-758fm/ff9fdb2f-380d-4e11-9b2b-37c44fa0290e?uuid=ff9fdb2f-380d-4e11-9b2b-37c44fa0290e"
            },
            {
              "state": "Online",
              "uri": "nvmf://10.224.0.9:8420/nqn.2019-05.io.openebs:/xfs-disk-pool/csi-7zjkf/be3adf14-e030-438a-9067-342366581c97?uuid=be3adf14-e030-438a-9067-342366581c97"
            },
            {
              "state": "Online",
              "uri": "nvmf://10.224.0.8:8420/nqn.2019-05.io.openebs:/xfs-disk-pool/csi-p7fnt/40dda88c-257c-454a-b8fd-77056989dfa9?uuid=40dda88c-257c-454a-b8fd-77056989dfa9"
            }
          ],
          "deviceUri": "nvmf://10.224.0.7:8420/nqn.2019-05.io.openebs:3e675149-2633-421c-b090-c6e4801a7f56",
          "node": "aks-storagepool-17429370-vmss000000",
          "rebuilds": 0,
          "protocol": "nvmf",
          "size": 107374182400,
          "state": "Online",
          "uuid": "628a3120-f46c-48ab-8bf2-9e1b7e4d8d8e"
        },
        "size": 107374182400,
        "status": "Online",
        "uuid": "3e675149-2633-421c-b090-c6e4801a7f56",
        "replica_topology": {
          "be3adf14-e030-438a-9067-342366581c97": {
            "node": "aks-storagepool-17429370-vmss000001",
            "pool": "csi-7zjkf",
            "state": "Online"
          },
          "40dda88c-257c-454a-b8fd-77056989dfa9": {
            "node": "aks-storagepool-17429370-vmss000002",
            "pool": "csi-p7fnt",
            "state": "Online"
          },
          "ff9fdb2f-380d-4e11-9b2b-37c44fa0290e": {
            "node": "aks-storagepool-17429370-vmss000000",
            "pool": "csi-758fm",
            "state": "Online"
          }
        }
      }
    }
  ],
  "next_token": 1
}
```

<!-- livebook:{"break_markdown":true} -->

**Notice**

* `/xfs-disk-pool/csi-7zjkf/` could be used to check md5 (see how to verify md5)

<!-- livebook:{"break_markdown":true} -->

1. Wait rebulding finished then check MD5 (check previous roadmap)

<!-- livebook:{"break_markdown":true} -->

1. Make sure after rebuilding the diskpools' size should be almost same
   ```
   zw@zwpdbh:~/download/azstore-add-ons$ kubectl get diskpool -A
   NAMESPACE   NAME                          CAPACITY         AVAILABLE        USED          RESERVED       READY   AGE
   acstor      manageddisk1-diskpool-mwoar   10992979238912   10911932907520   81046331392   184052908032   True    87m
   acstor      manageddisk2-diskpool-fwrfj   10992979238912   10911932907520   81046331392   184052908032   True    87m
   acstor      manageddisk3-diskpool-nqcwv   10992979238912   10911932907520   81046331392   184052908032   True    87m
   ```

<!-- livebook:{"break_markdown":true} -->

### Rebuilding VS Partial Rebuilding

Rebuilding

* Rebuilding on a single io-engine
* Rebuilding on multiple (e.g. 2) io-engines in parallel (a cluster with 4 or more nodes is needed)
* Bring down all nodes except the nexus node, then bring them up, delete pods/PVs, create pods/PVs

Partial rebuilding

* Partial rebuilding on a single io-engine
* Partial rebuilding on a single io-engine for multiple times
* Partial rebuilding on multiple (e.g. 2) io-engines in parallel (a cluster with 4 or more nodes is needed)

<!-- livebook:{"break_markdown":true} -->

Other notes:

* The finish of rebuilding is indicated by `state: Online`.

<!-- livebook:{"branch_parent_index":0} -->

## Others

* node - add label -> io-engione pod in acstor ns
* acstora extension -> storagepool -> diskpool -> pvc
* label node (targetnode) -> create pod (on targetnode) + pvc name
* Usually we don't need to create/specify pvc, the reason for us is because we want to use acstora extension's feature

<!-- livebook:{"branch_parent_index":0} -->

## Troubleshooting: helm install acstor-addon error

### Troubleshooting

Error: INSTALLATION FAILED: failed post-install: job failed: BackoffLimitExceeded

Solution:

* remove installed helm extension
* remove acstor namespace

```elixir
# To make sure extension status
"helm list -n acstor"
|> ExecCmd.run()
```

```elixir
# To uninstall failed extension
"helm uninstall acstor -n acstor"
|> ExecCmd.run()
```

```elixir
# Check if there is "acstor" namespace
"kubectl get namespace -A "
|> ExecCmd.run()
```

```elixir
# Delete "acstor" namespace
"kubectl delete namespace acstor "
|> ExecCmd.run()
```

```elixir
# Make sure there is no pod in acstor namespace
"kubectl get pods -n acstor"
|> ExecCmd.run()
```
