# Playground for Elixir Horizon project

## Remote Connection Test

Start elixir_horizion as

```sh
iex --name elixir_horizion@localhost --cookie some_token -S mix
```

<!-- livebook:{"break_markdown":true} -->

Start livebook container and set attached node

```sh
docker run \
--network=host \
-e LIVEBOOK_DISTRIBUTION=name \
-e LIVEBOOK_COOKIE=some_token \
-e LIVEBOOK_NODE=livebook@localhost \
-u $(id -u):$(id -g) \
-v $(pwd):/data \
ghcr.io/livebook-dev/livebook:0.8.1
```

Attach remote elixir_horizon node as:

* name: `elixir_horizion@localhost`
* cookie: `some_token`

<!-- livebook:{"break_markdown":true} -->

Let's test some code from next section.

<!-- livebook:{"branch_parent_index":0} -->

## NSP workflows

```elixir
alias Azure.Aks
Aks.module_info()
```

Fetch the lates 200 workflows

```elixir
Aks.update_latest_workflows(100)
Aks.list_workflows() |> length
```

Process NSP related workflows.

```elixir
defmodule Nsp do
  def filter_nsp_related_workflows(workflows) do
    workflows
    |> Enum.filter(fn %{"definitionName" => definition_name} ->
      definition_name
      |> String.downcase()
      |> String.contains?("scte")
    end)
  end

  def filter_finished_workflows(workflows) do
    workflows
    |> Enum.filter(fn x -> x.status != "Runnable" end)
  end

  def summary_from_workflow_detail(
        %{
          "data" => data_str,
          "definitionName" => definition_name,
          "id" => id,
          "status" => status,
          "createTime" => create_time_str
        } = workflow
      ) do
    %{
      "DeploymentName" => deployment_name,
      "TenantName" => tenant_name
      # "ScteDeploymentNameDst" => scte_deployment_name_dst,
      # "ScteDeploymentNameSrc" => scte_deployment_name_src,
    } = data_json = data_str |> Jason.decode!()

    common_parameters = %{
      id: id,
      definition_name: definition_name,
      deployment_name: deployment_name,
      tenant_name: tenant_name,
      status: status,
      create_time: How.Snippets.ConvertTime.get_datetime_in_shanghai_from_str(create_time_str)
    }

    special_parameters = data_json |> process_different_scte_deployment_name

    completion_time_str = Map.get(workflow, "completeTime", nil)

    completion_in_shanghai =
      How.Snippets.ConvertTime.get_datetime_in_shanghai_from_str(completion_time_str)

    common_parameters
    |> Map.merge(special_parameters)
    |> Map.merge(%{complete_time: completion_in_shanghai})
  end

  def summary_nsp_workflows(workflows) do
    workflows
    |> Enum.map(fn each -> summary_from_workflow_detail(each) end)
  end

  def process_different_scte_deployment_name(
        %{
          "ScteDeploymentName" => scte_deployment_name,
          "ScenarioId" => scte_id
        } = _data_json
      ) do
    %{scte_deployment_name: scte_deployment_name, scte_deployment: scte_id}
  end

  def process_different_scte_deployment_name(
        %{
          "DeploymentName" => scte_deployment_name,
          "ScenarioId" => scte_id
        } = _data_json
      ) do
    %{deployment_name_from_data: scte_deployment_name, scte_deployment: scte_id}
  end

  def process_different_scte_deployment_name(
        %{
          "ScteDeploymentNameDst" => scte_deployment_name_dst,
          "ScteDeploymentNameSrc" => scte_deployment_name_src,
          "ScenarioIdDst" => scte_dst_id,
          "ScenarioIdSrc" => scte_src_id
        } = _data_json
      ) do
    %{
      scte_deployment_name_dst: scte_deployment_name_dst,
      scte_deployment_name_src: scte_deployment_name_src,
      scte_deployment_dst: scte_dst_id,
      scte_deployment_src: scte_src_id
    }
  end

  def filter_nsp_copy_workloads(workloads) do
    workloads
    |> Enum.filter(fn x -> filter_nsp_copy_workload(x) end)
  end

  defp filter_nsp_copy_workload(%{scte_deployment_name_dst: _some} = workload) do
    workload
  end

  defp filter_nsp_copy_workload(%{} = _workload) do
    nil
  end
end
```

```elixir
top3 =
  Aks.list_workflows()
  |> Nsp.filter_nsp_related_workflows()
  |> Enum.take(3)

top3
```

```elixir
%{"data" => data_str} = top3 |> Enum.at(2)
data_str |> Jason.decode!()
```

```elixir
top3
|> Nsp.summary_nsp_workflows()
```

```elixir
Aks.list_workflows()
|> Nsp.filter_nsp_related_workflows()
|> Nsp.summary_nsp_workflows()
|> Nsp.filter_nsp_copy_workloads()
```

<!-- livebook:{"branch_parent_index":0} -->

## AKS workflows

```elixir
alias Azure.Aks
Aks.module_info()
```

```elixir
Aks.update_latest_workflows(200)
```

```elixir
Aks.list_aks_failed_workflows()
|> Aks.filter_workflows_after_date("2023-10-17")
|> Aks.summary_workflows()
```

```elixir
workflow_id = "5f09e7ad-2416-4381-9c70-7690b54d2d1d"
```

```elixir
workflow_id |> Aks.overwrite_default_k8s_config()
workflow_id |> Aks.get_workflow_from_id()
```

```elixir
ExecCmd.run("kubectl get pods")

"""
kubectl exec  xscn-workflow-pod -- sh -c "df -h /mnt/azuredisk"
"""
|> ExecCmd.run()
```

```elixir
"kubectl get pvc" |> ExecCmd.run()
```

```elixir
# Check the node status (such as os version)
ExecCmd.run("kubectl get node -o wide")
```

```elixir
ExecCmd.run("kubectl describe pvc xscn-workflow-pvc")
```

when you run `kubectl get po -n kube-system`, you're asking Kubernetes to retrieve and display information about all the pods in the kube-system namespace.

```elixir
# Check csi driver condition
ExecCmd.run("kubectl get po -n kube-system")
```

```elixir
# Check the win related node
ExecCmd.run("kubectl get po -n kube-system -o wide | grep csi | grep win")
```

```elixir
# export the kubeconfig file 
ExecCmd.run("cp ~/.kube/config /mnt/d/Downloads/b275941c-3dbb-49eb-bd8f-0fcdf6cb986b.config")
```

```elixir
Aks.list_aks_failed_workflows()
|> Aks.filter_workflows_after_date("2023-10-16")
|> Aks.summary_workflows()
|> Enum.each(fn each -> Aks.cleanup_aks_workflow(each) end)
```

```elixir
"""
kubectl delete --all pods && \
kubectl delete --all pvc && \
kubectl delete --all pv
"""
|> ExecCmd.run()
```

### Common commands to run during troubleshootings

##### Check pod status

kubectl get pods\
kubectl describe pod <pod-name>

##### Check Pvc status

kubectl get pvc

##### Check csi driver condition

kubectl get po -n kube-system | grep csi

##### Check nodes condition

kubectl get node -o wide

However, we better check nodes status from portal.

<!-- livebook:{"break_markdown":true} -->

### Common command to delete resources

```
kubectl delete --all pods
kubectl delete --all pvc
kubectl delete --all pv

kubectl delete --all pv --force

kubectl delete --all deployments
```

<!-- livebook:{"branch_parent_index":0} -->

## Collect metrics from VM Deployments

The task is to collect some usage metrics from log file of VM Deployment.

```elixir
csv_file = Path.join([File.cwd!(), "tmp/VM_deployment_Logs_2023_08_10_07_31.csv"])
File.exists?(csv_file)
```

Let's take a look what is log file look like:

```elixir
csv_file
|> File.stream!()
|> CSV.decode(separator: ?,, headers: true)
|> Enum.take(3)
```

```elixir
top3_messages =
  csv_file
  |> File.stream!()
  |> CSV.decode(separator: ?,, headers: true)
  |> Stream.map(fn {:ok, %{"Message" => message}} -> message end)
  |> Enum.take(3)
```

```elixir
example_message = top3_messages |> Enum.at(0)
```

Use Regex to capture the Caller we needed.

```elixir
%{"caller" => caller} =
  Regex.named_captures(~r/Caller: (?<caller>[a-zA-z0-9-]*)/, example_message)

caller
```

With everything we need, we could parse all log

```elixir
defmodule VmDeploymentLog do
  def summary_usage(csv_file) do
    csv_file
    |> File.stream!()
    |> CSV.decode(separator: ?,, headers: true)
    |> Stream.map(fn {:ok, %{"Message" => message}} -> message end)
    |> Stream.map(fn message -> parse_caller_from_message(message) end)
    |> Enum.reduce(%{}, &group_each_caller/2)
    |> Enum.sort_by(fn {_caller, n} -> n end, :desc)
  end

  def parse_caller_from_message(message) do
    %{"caller" => caller} = Regex.named_captures(~r/Caller: (?<caller>[a-zA-z0-9-.]*)/, message)
    # if caller == "" do
    #   IO.inspect(message)
    # end
    caller
  end

  def group_each_caller(caller, %{} = acc) do
    case Map.fetch(acc, caller) do
      {:ok, counter} ->
        Map.put(acc, caller, counter + 1)

      :error ->
        Map.put_new(acc, caller, 1)
    end
  end
end
```

```elixir
Path.join([File.cwd!(), "tmp/VM_deployment_Logs_2023_08_10_07_31.csv"])
|> VmDeploymentLog.summary_usage()
```

## Global setting for AKS clusters

```elixir
aks_list = [
  "Win-AKS-Dynamic-7yylv4",
  "k8s-dynamic-r4vxst",
  "AKS-Dynamic-02nct9",
  "Win-AKS-Static-j67zwh",
  "Win-AKS-Dynamic-7GLNB6",
  "Win-AKS-Static-wpo9fv",
  "Win-AKS-Dynamic-ygsyl7",
  "Win-AKS-Dynamic-gadpf2",
  "fixed-pocZ1KJVV"
]
```

### Azure Kubernetes Service CVE

For solving problem: [AzRel Red Flag- BlueCheese (Azure Kubernetes Service CVE)](https://microsoftapc.sharepoint.com/teams/RedFlag-AzureKubernetesServiceAKSCVE2/SitePages/Red-Flag--Azure-Kubernetes-Service-(AKS)-CVE.aspx)

Solution:

* [Upgrade Azure Kubernetes Service (AKS) node images](https://learn.microsoft.com/en-us/azure/aks/node-image-upgrade)

The following require us to open terminal and run "az-login" first (with JIT permission granted).

```elixir
ExecCmd.run("az account set --subscription 33922553-c28a-4d50-ac93-a5c682692168")
```

<!-- livebook:{"branch_parent_index":4} -->

## Check for available node image upgrades

```elixir
# List the node pool for a AKS cluster
aks_name = "Win-AKS-Dynamic-7yylv4"

{:ok, node_pools} =
  ExecCmd.run("az aks nodepool list --cluster-name #{aks_name} --resource-group #{aks_name}")
```

```elixir
node_pools =
  node_pools
  |> Jason.decode!()
  |> Enum.map(fn each -> %{id: Map.get(each, "id"), name: Map.get(each, "name")} end)
```

```elixir
# Check for available node image upgrades
# This will output the latest available node image for that node pool.
node_pool_name = "winpl"

ExecCmd.run("""
az aks nodepool get-upgrades \
  --nodepool-name #{node_pool_name} \
  --cluster-name #{aks_name} \
  --resource-group #{aks_name}
""")
```

The example output just shows `AKSWindows-2019-containerd-17763.4737.230809` as the `latestNodeImageVersion`.

```elixir
# Compare the latest version with your current node image version using the az aks nodepool show command.
ExecCmd.run("""
az aks nodepool show \
    --resource-group #{aks_name} \
    --cluster-name #{aks_name} \
    --name #{node_pool_name} \
    --query nodeImageVersion
""")
```

In this example, there is an available node image version upgrade, which is from version
`AKSWindows-2019-containerd-17763.4645.230712` to version `AKSWindows-2019-containerd-17763.4737.230809`.

<!-- livebook:{"branch_parent_index":4} -->

## Node OS auto-upgrade channel on existing cluster

```elixir
aks_list
|> Azure.Aks.enable_aks_node_pool_os_auto_upgrade()
```

<!-- livebook:{"branch_parent_index":4} -->

## Migrate Linux node pool to Mariner 2.0

```elixir
Azure.Aks.list_node_pools_for_aks_cluster("Win-AKS-Dynamic-7yylv4")
```

```elixir
%{ok: pools} = Azure.Aks.list_node_pools_for_aks_clusters(aks_list)
```

```elixir
pools
|> Enum.reject(fn %{nodeImageVersion: nodeImage} ->
  String.contains?(nodeImage |> String.downcase(), "windows")
end)
|> Enum.group_by(
  fn each -> each.aks end,
  fn each -> %{name: each.name, image: each.nodeImageVersion} end
)
|> Enum.filter(fn {_k, [v]} ->
  v.image |> String.downcase() |> String.contains?("mariner-v2gen2")
end)

# |> length()
```
